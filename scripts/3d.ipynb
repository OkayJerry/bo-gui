{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.optimize import rosen\n",
    "import torch\n",
    "from botorch.acquisition import UpperConfidenceBound\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils import standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_target_function(train_X, dtype=torch.float32, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    target_fn = torch.from_numpy(rosen(train_X)[..., None])\n",
    "    target_fn = target_fn.to(dtype=dtype, device=device)\n",
    "    return target_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BO_procedure(train_X, train_Y, bounds):\n",
    "    gp = SingleTaskGP(train_X, train_Y)\n",
    "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "    fit_gpytorch_model(mll)\n",
    "\n",
    "    UCB = UpperConfidenceBound(gp, beta=5, dtype=torch.float32)  # higher beta means more exploration (example: 10000)\n",
    "\n",
    "    candidate, acq_value = optimize_acqf(\n",
    "        UCB, bounds=bounds, q=1, num_restarts=5, raw_samples=20, dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    candidate_y = calculate_target_function(candidate)\n",
    "    # print(f'    Candidate: ({candidate[0][0].numpy()}, {candidate_y[0][0].numpy()})')\n",
    "\n",
    "    new_X = torch.cat([train_X, candidate])\n",
    "    new_Y = torch.cat([train_Y, candidate_y])\n",
    "\n",
    "    return gp, new_X, new_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(model, train_X, train_Y, bounds):\n",
    "    from matplotlib import pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    %matplotlib inline\n",
    "\n",
    "    target_x = torch.linspace(bounds[0][0], bounds[1][0], 101)\n",
    "    target_y = calculate_target_function(target_x, target_x)\n",
    "\n",
    "    # Initialize plot\n",
    "    ax = plt.subplot(111, projection='3d')\n",
    "    # test model on 101 regular spaced points on the interval [0, 1]\n",
    "    test_X = torch.linspace(bounds[0][0], bounds[1][0], 101)\n",
    "\n",
    "    \n",
    "\n",
    "    # no need for gradients\n",
    "    with torch.no_grad():\n",
    "        # plot target function (inclusion of noise decreases accuracy)\n",
    "        ax.plot(target_x.cpu().numpy(), target_y.cpu().numpy(), 'r')\n",
    "\n",
    "        # compute posterior\n",
    "        posterior = model.posterior(test_X)\n",
    "        # Get upper and lower confidence bounds (2 standard deviations from the mean)\n",
    "        lower, upper = posterior.mvn.confidence_region()\n",
    "        # Plot training points as black stars\n",
    "        ax.plot(train_X.cpu().numpy(), train_Y.cpu().numpy(), 'k*')\n",
    "        # Plot posterior means as blue line\n",
    "        ax.plot(test_X.cpu().numpy(), posterior.mean.cpu().numpy(), 'b')\n",
    "        # Shade between the lower and upper confidence bounds\n",
    "        ax.fill_between(test_X.cpu().numpy(), lower.cpu().numpy(), upper.cpu().numpy(), alpha=0.5)\n",
    "\n",
    "    ax.legend(['Target Function', 'Observed Data', 'Mean', 'Confidence'])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1., -1.],\n",
      "        [ 1.,  1.]])\n",
      "tensor([[4.],\n",
      "        [4.]])\n",
      "Executing Iteration 1:\n",
      "Executing Iteration 2:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/geraldhoskins/frib/bo-gui/scripts/3d.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geraldhoskins/frib/bo-gui/scripts/3d.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_ITERATIONS):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geraldhoskins/frib/bo-gui/scripts/3d.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mExecuting Iteration \u001b[39m\u001b[39m{\u001b[39;00mi \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/geraldhoskins/frib/bo-gui/scripts/3d.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     model, train_X, train_Y \u001b[39m=\u001b[39m BO_procedure(train_X, train_Y, BOUNDS)\n",
      "\u001b[1;32m/Users/geraldhoskins/frib/bo-gui/scripts/3d.ipynb Cell 5\u001b[0m in \u001b[0;36mBO_procedure\u001b[0;34m(train_X, train_Y, bounds)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/geraldhoskins/frib/bo-gui/scripts/3d.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m gp \u001b[39m=\u001b[39m SingleTaskGP(train_X, train_Y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/geraldhoskins/frib/bo-gui/scripts/3d.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mll \u001b[39m=\u001b[39m ExactMarginalLogLikelihood(gp\u001b[39m.\u001b[39mlikelihood, gp)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/geraldhoskins/frib/bo-gui/scripts/3d.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m fit_gpytorch_model(mll)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/geraldhoskins/frib/bo-gui/scripts/3d.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m UCB \u001b[39m=\u001b[39m UpperConfidenceBound(gp, beta\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)  \u001b[39m# higher beta means more exploration (example: 10000)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/geraldhoskins/frib/bo-gui/scripts/3d.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m candidate, acq_value \u001b[39m=\u001b[39m optimize_acqf(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/geraldhoskins/frib/bo-gui/scripts/3d.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     UCB, bounds\u001b[39m=\u001b[39mbounds, q\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, num_restarts\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, raw_samples\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/geraldhoskins/frib/bo-gui/scripts/3d.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/botorch/fit.py:130\u001b[0m, in \u001b[0;36mfit_gpytorch_model\u001b[0;34m(mll, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     sample_all_priors(mll\u001b[39m.\u001b[39mmodel)\n\u001b[1;32m    129\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     mll, _ \u001b[39m=\u001b[39m optimizer(mll, track_iterations\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    131\u001b[0m \u001b[39mexcept\u001b[39;00m NotPSDError:\n\u001b[1;32m    132\u001b[0m     retry \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/botorch/optim/fit.py:239\u001b[0m, in \u001b[0;36mfit_gpytorch_scipy\u001b[0;34m(mll, bounds, method, options, track_iterations, approx_mll, scipy_objective, module_to_array_func, module_from_array_func)\u001b[0m\n\u001b[1;32m    236\u001b[0m cb \u001b[39m=\u001b[39m store_iteration \u001b[39mif\u001b[39;00m track_iterations \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39mwith\u001b[39;00m gpt_settings\u001b[39m.\u001b[39mfast_computations(log_prob\u001b[39m=\u001b[39mapprox_mll):\n\u001b[0;32m--> 239\u001b[0m     res \u001b[39m=\u001b[39m minimize(\n\u001b[1;32m    240\u001b[0m         scipy_objective,\n\u001b[1;32m    241\u001b[0m         x0,\n\u001b[1;32m    242\u001b[0m         args\u001b[39m=\u001b[39;49m(mll, property_dict),\n\u001b[1;32m    243\u001b[0m         bounds\u001b[39m=\u001b[39;49mbounds,\n\u001b[1;32m    244\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    245\u001b[0m         jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    246\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    247\u001b[0m         callback\u001b[39m=\u001b[39;49mcb,\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    249\u001b[0m     iterations \u001b[39m=\u001b[39m []\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m track_iterations:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_minimize.py:699\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    697\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    700\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    701\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    702\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    703\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py:308\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         iprint \u001b[39m=\u001b[39m disp\n\u001b[0;32m--> 308\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(fun, x0, jac\u001b[39m=\u001b[39;49mjac, args\u001b[39m=\u001b[39;49margs, epsilon\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    309\u001b[0m                               bounds\u001b[39m=\u001b[39;49mnew_bounds,\n\u001b[1;32m    310\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39;49mfinite_diff_rel_step)\n\u001b[1;32m    312\u001b[0m func_and_grad \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mfun_and_grad\n\u001b[1;32m    314\u001b[0m fortran_int \u001b[39m=\u001b[39m _lbfgsb\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mintvar\u001b[39m.\u001b[39mdtype\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_optimize.py:263\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    259\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[1;32m    261\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m    264\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[1;32m    266\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl \u001b[39m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[1;32m    160\u001b[0m \u001b[39m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m callable(grad):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[1;32m     75\u001b[0m     \u001b[39m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/botorch/optim/utils.py:217\u001b[0m, in \u001b[0;36m_scipy_objective_and_grad\u001b[0;34m(x, mll, property_dict)\u001b[0m\n\u001b[1;32m    215\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmll(\u001b[39m*\u001b[39margs)\u001b[39m.\u001b[39msum()\n\u001b[1;32m    216\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 217\u001b[0m     \u001b[39mreturn\u001b[39;00m _handle_numerical_errors(error\u001b[39m=\u001b[39;49me, x\u001b[39m=\u001b[39;49mx)\n\u001b[1;32m    218\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    219\u001b[0m param_dict \u001b[39m=\u001b[39m OrderedDict(mll\u001b[39m.\u001b[39mnamed_parameters())\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/botorch/optim/utils.py:244\u001b[0m, in \u001b[0;36m_handle_numerical_errors\u001b[0;34m(error, x)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    239\u001b[0m     \u001b[39misinstance\u001b[39m(error, NanError)\n\u001b[1;32m    240\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msingular\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_message  \u001b[39m# old pytorch message\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minput is not positive-definite\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_message  \u001b[39m# since pytorch #63864\u001b[39;00m\n\u001b[1;32m    242\u001b[0m ):\n\u001b[1;32m    243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m\"\u001b[39m), np\u001b[39m.\u001b[39mfull_like(x, \u001b[39m\"\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 244\u001b[0m \u001b[39mraise\u001b[39;00m error\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/botorch/optim/utils.py:215\u001b[0m, in \u001b[0;36m_scipy_objective_and_grad\u001b[0;34m(x, mll, property_dict)\u001b[0m\n\u001b[1;32m    213\u001b[0m     output \u001b[39m=\u001b[39m mll\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39mtrain_inputs)\n\u001b[1;32m    214\u001b[0m     args \u001b[39m=\u001b[39m [output, train_targets] \u001b[39m+\u001b[39m _get_extra_mll_args(mll)\n\u001b[0;32m--> 215\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmll(\u001b[39m*\u001b[39;49margs)\u001b[39m.\u001b[39msum()\n\u001b[1;32m    216\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    217\u001b[0m     \u001b[39mreturn\u001b[39;00m _handle_numerical_errors(error\u001b[39m=\u001b[39me, x\u001b[39m=\u001b[39mx)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/gpytorch/module.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     32\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:64\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m     63\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlikelihood(function_dist, \u001b[39m*\u001b[39mparams)\n\u001b[0;32m---> 64\u001b[0m res \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39;49mlog_prob(target)\n\u001b[1;32m     65\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_other_terms(res, params)\n\u001b[1;32m     67\u001b[0m \u001b[39m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/gpytorch/distributions/multivariate_normal.py:147\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[1;32m    146\u001b[0m     \u001b[39mif\u001b[39;00m settings\u001b[39m.\u001b[39mfast_computations\u001b[39m.\u001b[39mlog_prob\u001b[39m.\u001b[39moff():\n\u001b[0;32m--> 147\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlog_prob(value)\n\u001b[1;32m    149\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args:\n\u001b[1;32m    150\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_sample(value)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/distributions/multivariate_normal.py:211\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args:\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_sample(value)\n\u001b[0;32m--> 211\u001b[0m diff \u001b[39m=\u001b[39m value \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc\n\u001b[1;32m    212\u001b[0m M \u001b[39m=\u001b[39m _batch_mahalanobis(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril, diff)\n\u001b[1;32m    213\u001b[0m half_log_det \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril\u001b[39m.\u001b[39mdiagonal(dim1\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, dim2\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mlog()\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# use a GPU if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = dtype=torch.float32\n",
    "\n",
    "NUM_ITERATIONS = 20\n",
    "BOUNDS = torch.tensor([[-1.0, -1.0], [1.0, 1.0]])\n",
    "x = torch.linspace(BOUNDS[0][0], BOUNDS[1][0], 2, dtype=DTYPE, device=DEVICE).unsqueeze(1)\n",
    "y = torch.linspace(BOUNDS[0][1], BOUNDS[1][1], 2, dtype=DTYPE, device=DEVICE).unsqueeze(1)\n",
    "train_X = torch.cat((x, y), 1)  # 0 for concatenate as row, 1 as column\n",
    "train_Y = calculate_target_function(train_X)\n",
    "print(train_X)\n",
    "print(train_Y)\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    print(f'Executing Iteration {i + 1}:')\n",
    "    model, train_X, train_Y = BO_procedure(train_X, train_Y, BOUNDS)\n",
    "\n",
    "# plot(model, train_X, train_Y, BOUNDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
